{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('TensorFlow25': conda)"
    },
    "interpreter": {
      "hash": "fe8cdd8b0905bc5dc811ca6f6f67baa88326482ee2d1fcebca307d9df55426eb"
    },
    "colab": {
      "name": "bpic11_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6Uw0Wav37irQ"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEwC3AN67irP"
      },
      "source": [
        "# Towards a Reliable and Interpretable Approach forBuilding Process Prediction Models\n",
        "\n",
        "Description: Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uw0Wav37irQ"
      },
      "source": [
        "## 0. Inport necessary library & Set workspace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixjTC1GQ7irQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041fca2b-32c2-449f-d026-9c14d23b01c1"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from natsort import natsorted\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "import math\n",
        "import itertools\n",
        "\n",
        "# library for plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils.data_utils import Sequence\n",
        "from keras.regularizers import l2\n",
        "from keras.constraints import non_neg, Constraint\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, confusion_matrix\n",
        "\n",
        "\n",
        "from keras.layers import Input, Concatenate, Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.recurrent import LSTM\n",
        "from tensorflow.keras.optimizers import Nadam, Adam, SGD, Adagrad\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "from tensorflow.keras import utils as ku \n",
        "from nltk.util import ngrams\n",
        "import pickle\n",
        "\n",
        "\n",
        "import keras\n",
        "print(keras.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MY_WORKSPACE_DIR = os.path.join(os.getcwd(),\"BPIC_11\")\n",
        "try:\n",
        "    os.makedirs(MY_WORKSPACE_DIR)\n",
        "    print(\"Directory \" , MY_WORKSPACE_DIR ,  \" created\")\n",
        "except:\n",
        "    print(\"Directory \" , MY_WORKSPACE_DIR ,  \" already exists\")\n",
        "\n",
        "OUTPUT_DIR = os.path.join(MY_WORKSPACE_DIR, \"output_files\")\n",
        "\n",
        "try:\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "    print(\"Directory \" , OUTPUT_DIR ,  \" created\")\n",
        "except:\n",
        "    print(\"Directory \" , OUTPUT_DIR ,  \" already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt3YdHdG7irR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2205d61-8ab0-4389-e1cb-6b7358de8b5a"
      },
      "source": [
        "\n",
        "\n",
        "# The args dictionary is adapted from GenerativeLSTM by Manuel Camargo\n",
        "# https://github.com/AdaptiveBProcess/GenerativeLSTM/blob/master/dg_training.py\n",
        "\n",
        "args = dict()\n",
        "\n",
        "args['processed_training_vec'] = os.path.join(OUTPUT_DIR, 'vec_training.p')\n",
        "args['processed_test_vec'] = os.path.join(OUTPUT_DIR, 'vec_test.p')\n",
        "args['weights'] = os.path.join(OUTPUT_DIR, 'weights.p')\n",
        "args['indexes'] = os.path.join(OUTPUT_DIR, 'indexes.p')\n",
        "args['args'] = os.path.join(OUTPUT_DIR, 'args.p') \n",
        "\n",
        "args['url'] = \"https://www.win.tue.nl/bpi/lib/exe/fetch.php?media=2011:hospital_log.csv.zip\"\n",
        "args['log_name'] = 'bpic2011_Hospital_Data'\n",
        "args['file_name'] = os.path.join(MY_WORKSPACE_DIR, 'BPIC_2011.csv') \n",
        "args['processed_file_name'] = os.path.join(OUTPUT_DIR, 'BPIC_2011_Processed.csv')\n",
        "args['task']='outcome'\n",
        "\n",
        "args['lstm_act'] = None # optimization function see keras doc\n",
        "args['dense_act'] = None # optimization function see keras doc\n",
        "args['n_size'] = 15 # n-gram size\n",
        "args['l_size'] = 50 # LSTM layer sizes\n",
        "args['norm_method'] = 'lognorm' # max, lognorm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_V8Ms_Z7irR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e43ea98-ed0e-4bad-bb22-888070318127"
      },
      "source": [
        "args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfKpTbDP7irS"
      },
      "source": [
        "## 1. Data Cleaning\n",
        "\n",
        "The [raw dataset](https://www.win.tue.nl/bpi/lib/exe/fetch.php?media=2011:hospital_log.csv.zip) is from Business Processing Intelligence Challenge (BPIC) 2011. Find more information [here](https://www.win.tue.nl/bpi/doku.php?id=2011:challenge)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptY7n9WU7irS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad86a4f1-4954-40a1-c9cd-0bb1ad33721b"
      },
      "source": [
        "# Dataframe creation\n",
        "raw_data = pd.read_csv(args['url'], compression='zip', sep=\";\", low_memory=False, dtype='str')\n",
        "raw_data.to_csv(path_or_buf=args['file_name'],sep=';', index=False)\n",
        "\n",
        "df = raw_data\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npE13yk87irS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "b544c380-54d9-4b63-fdc3-29c8a5776370"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TECr_WWf7irT"
      },
      "source": [
        "### Delete irrelevant attributes\n",
        "\n",
        "From the analysis of the columns for bpic11 and the standard of event log, we can divide attributes (columns) into `Trace` attributes and `Event` attributes. The `Trace` attributes share the same values in one case. While, the `Event` attributes keep changing between activities. Here list all the attribiutes for bpic11:\n",
        "\n",
        "- Attributes for traces (Static)\n",
        "    - case:concept:name\n",
        "    - Age, Age:1-15\n",
        "    - ~~Diagnosis, Diagnosis:1-15~~\n",
        "    - Diagnosis code, Diagnosis code:1-15\n",
        "    - Treatment code, Treatment code:1-15\n",
        "    - ~~Diagnosis Treatment Combination ID, Diagnosis Treatment Combination ID:1-15~~\n",
        "    - ~~Specialism code, Specialism code:1-15~~\n",
        "    - ~~Start date, Start date:1-15~~\n",
        "    - ~~End date, End date:1-15~~\n",
        "\n",
        "- Attributes for events (Dynamic)\n",
        "    - org:group\n",
        "    - Number of executions\n",
        "    - ~~Specialism code~~\n",
        "    - event:concept:name\n",
        "    - Producer code\n",
        "    - Section\n",
        "    - Activity code\n",
        "    - time:timestamp\n",
        "    - ~~lifecycle:transition~~\n",
        "\n",
        "All irrelevant attributes with delete line need to be droped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs1pV8ox7irT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e94e93b-6efd-48f9-d265-046c8c54bba8"
      },
      "source": [
        "# Clean all columns with \"Diagnosis\", \"Diagnosis Treatment Combination ID\", \"Specialism code\" and \"lifecycle:transition\"\n",
        "df = df.loc[:,~df.columns.str.startswith('Diagnosis:')]\n",
        "df = df.drop(columns='Diagnosis')\n",
        "df = df.loc[:,~df.columns.str.startswith('Specialism code')] \n",
        "df = df.loc[:,~df.columns.str.startswith('lifecycle:transition')]\n",
        "df = df.loc[:,~df.columns.str.startswith('Diagnosis Treatment Combination ID')]\n",
        "\n",
        "df = df.loc[:,~df.columns.str.startswith('Start date')] \n",
        "df = df.loc[:,~df.columns.str.startswith('End date')]\n",
        "\n",
        "df = df.drop(columns='Unnamed: 127')\n",
        "df.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3XVCQNj7irU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "ad1af02c-ccdc-453c-f684-57bb7ca5d7e1"
      },
      "source": [
        "df = df.astype({'case:concept:name': 'int64', 'Number of executions': 'int64'})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQw0H4uu7irU"
      },
      "source": [
        "### Combine repeating features to a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQjOwgaq7irU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "6f949081-ac9c-48cb-c7e1-02943d9ead84"
      },
      "source": [
        "# Sort all columns in alphabetical order\n",
        "\n",
        "df = df.reindex(natsorted(df.columns), axis=1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hod1A5Fq7irU"
      },
      "source": [
        "From the previous analysis, only three static `Trace` attributes - `Age`, `Diagnosis code` and `Treatment code` - contain repeating columns. Here we use functions for combining the repearting features into lists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOG9tZFc7irV"
      },
      "source": [
        "For three feature lists, we apply different methods to choose a suitable value.\n",
        "\n",
        "- `Age`: Because the diagnosis and treatment for a patient may be over years, the age in one case may increase. Here we choose the latest one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHCstwpC7irV"
      },
      "source": [
        "# function for finding last valid value\n",
        "def findLastValid(x):\n",
        "    list_of_last_value = []\n",
        "    for index, row in x.iterrows():\n",
        "        if row.last_valid_index() is None:\n",
        "            list_of_last_value.append(np.nan)\n",
        "        else:\n",
        "            list_of_last_value.append(row[row.last_valid_index()])\n",
        "    \n",
        "    return list_of_last_value\n",
        "\n",
        "Age_List = findLastValid(df.loc[:, df.columns.str.startswith('Age')])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-um381OA7irV"
      },
      "source": [
        "\n",
        "- `Diagnosis code`: This code shows the diagnosis from doctors. In the Appendix of bpic11 winner paper by Bose and Aalst, it provides a table for distribution of cases based on diagnosis code combinations. We choose top 8 combinations ({'M13'},{'M16'},{'M11'},{'M14'},{'106'},{'822', '106'}, {'M13', '106'}, {'M13', '822', '106'}) for our experimentation in order to avoid noise and unbalanced splits. \n",
        "\n",
        "```bib\n",
        "@inproceedings{bpic11_winner,\n",
        "  title={Analysis of patient treatment procedures: The BPI Challenge case study},\n",
        "  author={R. P. J. C. Bose and W. Aalst},\n",
        "  year={2011}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rud2QhOX7irV"
      },
      "source": [
        "# function for finding the valid value list\n",
        "\n",
        "def findValidList(x):\n",
        "    list_of_values = []\n",
        "    for index, row in x.iterrows():\n",
        "        values = []\n",
        "        for i, v in row.items():\n",
        "            if pd.notnull(v):\n",
        "                values.append(v)\n",
        "        # values = frozenset(values)\n",
        "        list_of_values.append(values)\n",
        "    return list_of_values\n",
        "\n",
        "DiagnosisCode_List = findValidList(df.loc[:, df.columns.str.startswith('Diagnosis code')])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP1l9DhP7irW"
      },
      "source": [
        "for index, value in enumerate(DiagnosisCode_List):\n",
        "    DiagnosisCode_List[index] = list(set(value))\n",
        "    DiagnosisCode_List[index].sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BG2AExV7irW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32026b6c-2a1b-48a0-918b-061f04b91185"
      },
      "source": [
        "DiagnosisCode_Unique = [list(y) for y in set([tuple(x) for x in DiagnosisCode_List])]\n",
        "DiagnosisCode_Unique"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hCQ1qrP7irW"
      },
      "source": [
        "# ['106', '822'] -> ['106']\n",
        "# ['106', 'M13'] -> ['106']\n",
        "# ['106', '822', 'M13'] -> ['106']\n",
        "\n",
        "Changed_Diag_Comb = [['106', '822'], ['106', 'M13'], ['106', '822', 'M13']]\n",
        "\n",
        "for index, value in enumerate(DiagnosisCode_List):\n",
        "    if value in Changed_Diag_Comb:\n",
        "        DiagnosisCode_List[index] = ['106']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOvae5MS7irX"
      },
      "source": [
        "- `Treatment code`: Combine the repeated codes in to one list and find a most frquent one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuCFAv7P7irX"
      },
      "source": [
        "# function for finding most frequent element\n",
        "def mostFrequent(x):\n",
        "    list_of_values=[]\n",
        "    for case in x:\n",
        "        if len(case) == 0:\n",
        "            list_of_values.append(None)\n",
        "        else:\n",
        "            list_of_values.append(max(case, key = case.count))\n",
        "    return list_of_values\n",
        "\n",
        "TreatmentCode_List = findValidList(df.loc[:, df.columns.str.startswith('Treatment code')])\n",
        "TreatmentCode_List = mostFrequent(TreatmentCode_List)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ygk2qA7irX"
      },
      "source": [
        "### Add Additinal Features\n",
        "\n",
        "Add time features, by calculating the time difference between the first event and the last event."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6ke-1077irX"
      },
      "source": [
        "# Calculate week and day\n",
        "\n",
        "# total number of traces: 1143 (Hard coding)\n",
        "\n",
        "Month_list = []\n",
        "Day_list = []\n",
        "tbtw_list = []\n",
        "\n",
        "for case_id in range(1143):\n",
        "    time_df = df.loc[df['case:concept:name'] == case_id]\n",
        "    dateStart = datetime.strptime(time_df.iloc[0,-1], '%Y-%m-%dT%H:%M:%S%z')\n",
        "    dateEnd = datetime.strptime(time_df.iloc[-1,-1], '%Y-%m-%dT%H:%M:%S%z')\n",
        "    Day_list.append(abs((dateEnd - dateStart).days))\n",
        "    Month_list.append(abs((dateEnd.year - dateStart.year) * 12 + (dateEnd.month - dateStart.month)))\n",
        "\n",
        "    for index, row in time_df.iterrows():\n",
        "        dateNow = datetime.strptime(row['time:timestamp'], '%Y-%m-%dT%H:%M:%S%z')\n",
        "        tbtw_list.append(abs((dateNow - dateStart).days))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BCE2kqY7irX"
      },
      "source": [
        "The new dataframe should include these feaures: \n",
        "\n",
        "Activity, Department, Number of executions, Activitycode, Producer code, Section, Age, Diagnosis Code, Treatment code and Year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uufp1yZb7irY"
      },
      "source": [
        "processed_df = df[['case:concept:name', 'event:concept:name', 'org:group', 'time:timestamp', 'Activity code', 'Number of executions', 'Producer code', 'Section']]\n",
        "\n",
        "processed_df = processed_df.assign(Age = Age_List)\n",
        "processed_df = processed_df.assign(Diagnosis_code = DiagnosisCode_List)\n",
        "processed_df = processed_df.assign(Treatment_code = TreatmentCode_List)\n",
        "processed_df = processed_df.assign(tbtw = tbtw_list)\n",
        "\n",
        "def input_day(row):\n",
        "    return Day_list[row['case:concept:name']]\n",
        "\n",
        "def input_month(row):\n",
        "    return Month_list[row['case:concept:name']]\n",
        "\n",
        "# case lenth\n",
        "case_len_list = processed_df.groupby(['case:concept:name']).size().values\n",
        "def input_cl(row):\n",
        "    return case_len_list[row['case:concept:name']]\n",
        "\n",
        "processed_df['Total month'] = processed_df.apply(lambda x: input_month(x), axis=1)\n",
        "processed_df['Total day'] = processed_df.apply(lambda x: input_day(x), axis=1)\n",
        "\n",
        "processed_df['Case Lenth'] = processed_df.apply(lambda x: input_cl(x), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PCwW5RA7irY"
      },
      "source": [
        "### Clean nosie in Diagnosis_code and event:concept:name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k-cChJ97irY"
      },
      "source": [
        "# clean Number of executions less than 1\n",
        "\n",
        "processed_df = processed_df[processed_df['Number of executions'] > 0]\n",
        "\n",
        "activity_list = processed_df['event:concept:name'].value_counts().loc[lambda x : x>19].index.tolist()\n",
        "processed_df = processed_df[processed_df['event:concept:name'].isin(activity_list)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twrE-01p7irY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bd3cbc-5138-4945-b92f-07e46f6b1640"
      },
      "source": [
        "# clean Treatment_code is null\n",
        "processed_df = processed_df[~processed_df['Treatment_code'].isnull()]\n",
        "processed_df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r5pvpoj7irZ"
      },
      "source": [
        "processed_df['Diagnosis_code'] = processed_df['Diagnosis_code'].apply(lambda x: ','.join(map(str, x)))\n",
        "Selected_Diag_Comb = ['M13', 'M16', 'M11', 'M14', '106']\n",
        "processed_df = processed_df[processed_df['Diagnosis_code'].isin(Selected_Diag_Comb)]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DbUnKAd7irZ"
      },
      "source": [
        "column_names = ['CaseID', 'Activity', 'Department', 'Timestamps', 'Activity code', 'Number of executions', 'Producer code', 'Section', 'Age', 'Diagnosis code', 'Treatment code', 'tbtw', 'Month', 'Day', 'Case Lenth']\n",
        "processed_df.columns = column_names\n",
        "\n",
        "processed_df = processed_df.astype({'Age': 'int64'})\n",
        "\n",
        "processed_df = processed_df.reset_index(drop=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF6UY_bM7irZ"
      },
      "source": [
        "### Save processed dataframe to csv for backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25MFT-Ev7irZ"
      },
      "source": [
        "processed_df.to_csv(path_or_buf=args['processed_file_name'],sep=';', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3elwx6uJ7irZ"
      },
      "source": [
        "## 2. Understanding the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDHonnj07irZ"
      },
      "source": [
        "### Loading the data and parameter files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfdBQdQ77ira",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "cf72298c-e139-48f4-8a80-9ccb9d0e07c2"
      },
      "source": [
        "log_df = processed_df\n",
        "log_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3rNo53v7ira"
      },
      "source": [
        "### Data Analysis - Balance of Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWh5VmDD7ira",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac69bda-fd1e-4cf6-c468-200b1dbb89ec"
      },
      "source": [
        "#Checking the Balance of the Dataset, by the target variable\n",
        "print('Distribution of cases by the target variable - Diagnosis code\\n')\n",
        "print(log_df.groupby(['Diagnosis code'])['CaseID'].nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQLsJ8g47ira"
      },
      "source": [
        "### Create indexes\n",
        "Author: Renuka Sindagatta/ Manuel Camargo\n",
        "\n",
        "Function: creates an index (index encoded set) for a given categorical column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmNrh1hM7ira"
      },
      "source": [
        "def create_index(log_df, column):\n",
        "    \"\"\"Creates an idx for a categorical attribute.\n",
        "    Args:\n",
        "        log_df: dataframe.\n",
        "        column: column name.\n",
        "    Returns:\n",
        "        index of a categorical attribute pairs.\n",
        "    \"\"\"\n",
        "    temp_list = log_df[[column]].values.tolist()\n",
        "    subsec_set = {(x[0]) for x in temp_list}\n",
        "    subsec_set = sorted(list(subsec_set))\n",
        "    alias = dict()\n",
        "    if column !='Diagnosis code':\n",
        "      for i, _ in enumerate(subsec_set):\n",
        "          alias[subsec_set[i]] = i + 1\n",
        "    else:\n",
        "      for i, _ in enumerate(subsec_set):\n",
        "          alias[subsec_set[i]] = i  \n",
        "    return alias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy4CsNvw7ira"
      },
      "source": [
        "create the indexes for the processed dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rZfrLfv7ira",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "fdc76f1a-085f-4ae5-cb5f-d8771077ab16"
      },
      "source": [
        "# Index creation for activity\n",
        "# column_names = ['CaseID', 'Activity', 'Department', 'Timestamps', 'Activity code', 'Number of executions', 'Producer code', 'Section', 'Age', 'Diagnosis code', 'Treatment code', 'Month', 'Day']\n",
        "\n",
        "ac_index = create_index(log_df, 'Activity')\n",
        "ac_index['start'] = 0\n",
        "ac_index['end'] = len(ac_index)\n",
        "index_ac = {v: k for k, v in ac_index.items()}\n",
        "\n",
        "# Index creation for department/role\n",
        "\n",
        "rl_index = create_index(log_df, 'Department')\n",
        "rl_index['start'] = 0\n",
        "rl_index['end'] = len(rl_index)\n",
        "index_rl = {v: k for k, v in rl_index.items()}\n",
        "\n",
        "# Index creation for Diagnosis\n",
        "\n",
        "di_index = create_index(log_df, 'Diagnosis code')\n",
        "\n",
        "index_di = {v: k for k, v in di_index.items()}\n",
        "\n",
        "# Index creation for Treatment\n",
        "tr_index = create_index(log_df, 'Treatment code')\n",
        "tr_index['start'] = 0\n",
        "tr_index['end'] = len(tr_index)\n",
        "index_tr = {v: k for k, v in tr_index.items()}\n",
        "\n",
        "#mapping the dictionary values as columns in the dataframe\n",
        "log_df['ac_index'] = log_df['Activity'].map(ac_index)\n",
        "log_df['rl_index'] = log_df['Department'].map(rl_index)\n",
        "log_df['di_index'] = log_df['Diagnosis code'].map(di_index)\n",
        "log_df['tr_index'] = log_df['Treatment code'].map(tr_index)\n",
        "\n",
        "print(rl_index)\n",
        "print(index_rl)\n",
        "log_df.head()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X8DrDSp7irb"
      },
      "source": [
        "### Data Analysis - Correlation between features\n",
        "\n",
        "credits: https://seaborn.pydata.org/examples/many_pairwise_correlations.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1Heu9IU7irb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "7246626d-3545-47a2-8bf2-3f0e5dd80733"
      },
      "source": [
        "cor_columns = ['Age','Month', 'tbtw', 'Case Lenth','ac_index','rl_index','tr_index','di_index']\n",
        "\n",
        "d = log_df[cor_columns]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr = d.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpY6yLuu7irb"
      },
      "source": [
        "### Split train test\n",
        "\n",
        "Author: Renuka Sindagatta/ Manuel Camargo\n",
        "\n",
        "Function: divides the dataset into test and train sets, preserving traces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BBKetl_7irb"
      },
      "source": [
        "# =============================================================================\n",
        "# Split an event log dataframe to peform split-validation \n",
        "# =============================================================================\n",
        "def split_train_test(df, percentage):\n",
        "    cases = df.CaseID.unique()\n",
        "    num_test_cases = int(np.round(len(cases)*percentage))\n",
        "    test_cases = cases[:num_test_cases]\n",
        "    train_cases = cases[num_test_cases:]\n",
        "    df_train, df_test = pd.DataFrame(), pd.DataFrame()\n",
        "    for case in train_cases:\n",
        "        df_train = df_train.append(df[df.CaseID==case]) \n",
        "    df_train = df_train.sort_values('Timestamps', ascending=True).reset_index(drop=True)\n",
        " \n",
        "    for case in test_cases:\n",
        "        df_test = df_test.append(df[df.CaseID==case]) \n",
        "    df_test = df_test.sort_values('Timestamps', ascending=True).reset_index(drop=True)\n",
        "    \n",
        "    return df_train, df_test "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bcmakde7irb"
      },
      "source": [
        "splitting the dataframe into test and train sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5nC8zdR7irb"
      },
      "source": [
        "# Split validation datasets\n",
        "log_df_train, log_df_test = split_train_test(log_df, 0.3) # 70%/30%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFaLi17n7irc"
      },
      "source": [
        "### normalize_events\n",
        "\n",
        "Modified from the function of: Renuka Sindagatta/ Manuel Camargo\n",
        "\n",
        "Function: Normalizes the numerical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Vmp4XF7irc"
      },
      "source": [
        "def normalize_events(log_df,args,features):\n",
        "\n",
        "#log_df(DataFrame): The dataframe with eventlog data\n",
        "#args(Dictionary): The set of parameters\n",
        "#Returns a Dataframe with normalized numerical features\n",
        "  for feature in features:\n",
        "    if args['norm_method'] == 'max':\n",
        "        mean_feature = np.mean(log_df.feature)\n",
        "        std_feature = np.std(log_df.feature)\n",
        "        norm = lambda x: (x[feature]-mean_feature)/std_feature\n",
        "        log_df['%s_norm'%(feature)] = log_df.apply(norm, axis=1)\n",
        "    elif args['norm_method'] == 'lognorm':\n",
        "        logit = lambda x: math.log1p(x[feature])\n",
        "        log_df['%s_log'%(feature)] = log_df.apply(logit, axis=1)\n",
        "        mean_feature = np.mean(log_df['%s_log'%(feature)])\n",
        "        std_feature=np.std(log_df['%s_log'%(feature)])\n",
        "        norm = lambda x: (x['%s_log'%(feature)]-mean_feature)/std_feature\n",
        "        log_df['%s_norm'%(feature)] = log_df.apply(norm, axis=1)\n",
        "  return log_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gje4T-iv7irc"
      },
      "source": [
        "Adding normalized features: to the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar4NMguu7irc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "e0554545-0219-4cd2-ac97-7020b8aa6791"
      },
      "source": [
        "numerical_features = ['Age','Month', 'Day', 'tbtw', 'Case Lenth']\n",
        "log_df_train = normalize_events(log_df_train,args,numerical_features)\n",
        "log_df_test = normalize_events(log_df_test,args,numerical_features)\n",
        "log_df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVNgcKAk7irc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cf48b8-c901-4f3c-ca3d-027b2da4d41e"
      },
      "source": [
        "training_traces = len(log_df_train['CaseID'].unique())\n",
        "test_traces = len(log_df_test['CaseID'].unique())\n",
        "\n",
        "print('Number of traces in training set is:'+str(training_traces))\n",
        "print('Number of traces in test set is:'+str(test_traces))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwo3Lkho7irc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad813238-ff89-4dc8-e3c8-7945aff22391"
      },
      "source": [
        "#Checking the Balance of the Dataset, by the target variable\n",
        "print('training dataset')\n",
        "print(log_df_train.groupby(['Diagnosis code'])['CaseID'].nunique())\n",
        "\n",
        "print('test dataset')\n",
        "print(log_df_test.groupby(['Diagnosis code'])['CaseID'].nunique())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM0UDQl87ird"
      },
      "source": [
        "### Reformat events\n",
        "\n",
        "Modified from the function of: Renuka Sindagatta/ Manuel Camargo\n",
        "\n",
        "Function: converts the dataframe into a dictionary, using the indexes created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekKIV-zp7ird"
      },
      "source": [
        "# ==============================================================================\n",
        "# Reformat events: converts the dataframe into a numerical dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "def reformat_events(log_df, ac_index, rl_index,di_index):\n",
        "    \"\"\"Creates series of activities, roles and relative times per trace.\n",
        "    Args:\n",
        "        log_df: dataframe.\n",
        "        ac_index (dict): index of activities.\n",
        "        rl_index (dict): index of roles.\n",
        "    Returns:\n",
        "        list: lists of activities, roles and relative times.\n",
        "    \"\"\"\n",
        "    log_df = log_df.to_dict('records')\n",
        "\n",
        "    temp_data = list()\n",
        "    log_df = sorted(log_df, key=lambda x: (x['CaseID'], x['Timestamps']))\n",
        "    for key, group in itertools.groupby(log_df, key=lambda x: x['CaseID']):\n",
        "        trace = list(group)\n",
        "        #dynamic features\n",
        "        ac_order = [x['ac_index'] for x in trace]\n",
        "        rl_order = [x['rl_index'] for x in trace]\n",
        "        tbtw = [x['tbtw_norm'] for x in trace]\n",
        "\n",
        "\n",
        "        #static features: the aggregation used is max(), however, any aggregation could be used since we have a single value for this for the whole trace\n",
        "        age = max(x['Age_norm'] for x in trace)\n",
        "        months = max(x['Month_norm'] for x in trace)\n",
        "        case_len = max(x['Case Lenth_norm'] for x in trace)\n",
        "        treatment = max(x['tr_index'] for x in trace)\n",
        "\n",
        "        #outcome\n",
        "        diagnosis = max(x['di_index'] for x in trace)\n",
        "\n",
        "        #Adding start and end to the dynamic features\n",
        "        ac_order.insert(0, ac_index[('start')])\n",
        "        ac_order.append(ac_index[('end')])\n",
        "        rl_order.insert(0, rl_index[('start')])\n",
        "        rl_order.append(rl_index[('end')])\n",
        "        tbtw.insert(0, 0)\n",
        "        tbtw.append(0)\n",
        "        temp_dict = dict(caseid=key,\n",
        "                         ac_order=ac_order,\n",
        "                         rl_order=rl_order,\n",
        "                         tbtw=tbtw,\n",
        "                         age=age,\n",
        "                         treatment=treatment,\n",
        "                         months=months,\n",
        "                         case_len=case_len,\n",
        "                         diagnosis=diagnosis)\n",
        "        temp_data.append(temp_dict)\n",
        "\n",
        "    return temp_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQaZQMjC7ird"
      },
      "source": [
        "converting the training dataframe into a dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAzVmUw07ird"
      },
      "source": [
        "log_train = reformat_events(log_df_train, ac_index, rl_index,di_index)\n",
        "log_test = reformat_events(log_df_test, ac_index, rl_index,di_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX_ueoOQ7ird",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65858944-04aa-495c-93a1-a1569a4b0f04",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "#print a sample of the dictionary\n",
        "print(log_train[111])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1nqPsbD7ird"
      },
      "source": [
        "###  Vectorization\n",
        "\n",
        "Author: Bemali Wickramanayake\n",
        "\n",
        "Inspired by the code of: Renuka Sindagatta\n",
        "\n",
        "Function: Creating the Input and Output Tensors \n",
        "\n",
        "Notes: Editing needs to finish for this function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AX2NYIX7ire"
      },
      "source": [
        "# Support function for Vectirization\n",
        "\n",
        "# This function returns the maximum trace length (trc_len), and the number of cases for train and test sets (cases)\n",
        "# The maximum out of trc_len for train and test sets will be used to define the trace length of the dataset that is fed to lstm\n",
        "\n",
        "def lengths (log):\n",
        "  trc_len = 1\n",
        "  cases = 1\n",
        "\n",
        "  for i,_ in enumerate(log):\n",
        "\n",
        "    if trc_len <len(log[i]['ac_order']):\n",
        "\n",
        "        trc_len = len(log[i]['ac_order'])\n",
        "        cases += 1\n",
        "    else:\n",
        "        cases += 1\n",
        "\n",
        "  return trc_len, cases\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYooahJS7ire",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52a1d57-1c56-472f-df2c-9188988d26ac"
      },
      "source": [
        "#Obtain the trc_len and cases for each set\n",
        "\n",
        "trc_len_train, cases_train = lengths(log_train)\n",
        "trc_len_test, cases_test = lengths(log_test)\n",
        "\n",
        "trc_len = trc_len_train\n",
        "if trc_len < trc_len_test:\n",
        "  trc_len = trc_len_test\n",
        "\n",
        "print(\"trace_length: \"+str(trc_len)+\", training cases: \"+str(cases_train)+\", test cases: \"+str(cases_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6h4W7SY7ire"
      },
      "source": [
        "def vectorization(log, ac_index, rl_index, di_index, tr_index, trc_len,cases):\n",
        "\n",
        "#Example function with types documented in the docstring.\n",
        "#Args:\n",
        "        #log: event log data in a dictionary.\n",
        "        #ac_index (dict): index of activities.\n",
        "        #rl_index (dict): index of roles (departments).\n",
        "        #di_index (dict) : index of diagnosis codes.\n",
        "\n",
        "#Returns:vec: Dictionary that contains all the LSTM inputs. \"\"\"\n",
        "\n",
        "  vec = {'prefixes':dict(), 'static':dict(),'diagnosis':[]} \n",
        "  len_ac = trc_len  \n",
        "\n",
        "  for i ,_ in enumerate(log):\n",
        "  \n",
        "    padding = np.zeros(len_ac-len(log[i]['ac_order']))\n",
        "\n",
        "    if i == 0:\n",
        "            vec['prefixes']['x_ac_inp'] = np.array(np.append(log[i]['ac_order'],padding))\n",
        "            vec['prefixes']['x_rl_inp'] = np.array(np.append(log[i]['rl_order'],padding))\n",
        "            vec['prefixes']['xt_inp'] = np.array(np.append(log[i]['tbtw'],padding))\n",
        "            vec['static']['x_age_inp'] = np.array(log[i]['age'])\n",
        "            vec['static']['x_months_inp'] = np.array(log[i]['months'])\n",
        "            vec['static']['x_cl_inp'] = np.array(log[i]['case_len'])\n",
        "            vec['static']['x_tr_inp'] = np.array(log[i]['treatment'])\n",
        "            vec['diagnosis'] = np.array(log[i]['diagnosis'])\n",
        "\n",
        "\n",
        "            #print(len(vec['prefixes']['x_ac_inp']))\n",
        "\n",
        "  \n",
        "    vec['prefixes']['x_ac_inp'] = np.concatenate((vec['prefixes']['x_ac_inp'],\n",
        "                                                          np.array(np.append(log[i]['ac_order'],padding))), axis=0)\n",
        "    vec['prefixes']['x_rl_inp'] = np.concatenate((vec['prefixes']['x_rl_inp'],\n",
        "                                                          np.array(np.append(log[i]['rl_order'],padding))), axis=0)\n",
        "    vec['prefixes']['xt_inp'] = np.concatenate((vec['prefixes']['xt_inp'],\n",
        "                                                        np.array(np.append(log[i]['tbtw'],padding))), axis=0)\n",
        "    vec['static']['x_age_inp'] = np.append(vec['static']['x_age_inp'],log[i]['age'])\n",
        "    vec['static']['x_months_inp'] = np.append(vec['static']['x_months_inp'],log[i]['months'])\n",
        "    vec['static']['x_cl_inp'] = np.append(vec['static']['x_cl_inp'],log[i]['case_len'])\n",
        "    vec['static']['x_tr_inp'] = np.append(vec['static']['x_tr_inp'],log[i]['treatment'])\n",
        "    vec['diagnosis'] = np.append(vec['diagnosis'],log[i]['diagnosis'])\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "  #The concatenation returns a flattened vector. Hence, reshaping the vectors at the end\n",
        "  vec['prefixes']['x_ac_inp'] = np.reshape(vec['prefixes']['x_ac_inp'],(cases,len_ac))\n",
        "  vec['prefixes']['x_rl_inp'] = np.reshape(vec['prefixes']['x_rl_inp'],(cases,len_ac))\n",
        "  vec['prefixes']['xt_inp'] = np.reshape(vec['prefixes']['xt_inp'],(cases,len_ac))\n",
        "\n",
        "  #one-hot-encoding the y class\n",
        "  vec['diagnosis'] = ku.to_categorical(vec['diagnosis'],\n",
        "                                               num_classes=len(di_index))\n",
        "\n",
        "  return vec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psfkTOQN7ire"
      },
      "source": [
        "converting the training log (dictionary) into a Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTRbJuxC7ire"
      },
      "source": [
        "vec_train = vectorization(log_train,ac_index,rl_index,di_index,tr_index,trc_len,cases_train)\n",
        "vec_test = vectorization(log_test,ac_index,rl_index,di_index,tr_index,trc_len,cases_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vkzeH3Y7ire",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e03600c-1dcf-45ac-fbb7-139318f4d4a4"
      },
      "source": [
        "vec_train['diagnosis']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRNFzPxP7irf"
      },
      "source": [
        "### Initial Embedding weights for the Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USa2ObZd7irf"
      },
      "source": [
        "# Load embedded matrix\n",
        "ac_weights = ku.to_categorical(sorted(index_ac.keys()), len(ac_index))\n",
        "#print('AC_WEIGHTS', ac_weights)\n",
        "rl_weights =  ku.to_categorical(sorted(index_rl.keys()), len(rl_index))\n",
        "#print('RL_WEIGHTS', rl_weights)\n",
        "tr_weights =  ku.to_categorical(sorted(index_tr.keys()), len(tr_index))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcyixbWB7irf"
      },
      "source": [
        "### Saving the Processed Tensor and Other Support Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWeBjssr7irf"
      },
      "source": [
        "# args['processed_training_vec'] = vec_train\n",
        "# args['processed_test_vec'] = vec_test\n",
        "\n",
        "\n",
        "# # converting the weights into a dictionary and saving\n",
        "# weights = {'ac_weights':ac_weights, 'rl_weights':rl_weights, 'diagnoses':len(di_index)}\n",
        "# args['weights'] = weights\n",
        "\n",
        "# indexes = {'index_ac':index_ac, 'index_rl':index_rl, 'index_di':index_di, 'index_tr':index_tr}\n",
        "# args['indexes'] = indexes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSqUKMNR7irf"
      },
      "source": [
        "\n",
        "# saving the processed tensor\n",
        "with open(args['processed_training_vec'], 'wb') as fp:\n",
        "    pickle.dump(vec_train, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open(args['processed_test_vec'], 'wb') as fp:\n",
        "    pickle.dump(vec_test, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# converting the weights into a dictionary and saving\n",
        "weights = {'ac_weights':ac_weights, 'rl_weights':rl_weights, 'diagnoses':len(di_index), 'tr_weights':tr_weights}\n",
        "with open(args['weights'], 'wb') as fp:\n",
        "    pickle.dump(weights, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# converting the weights into a dictionary and saving\n",
        "indexes = {'index_ac':index_ac, 'index_rl':index_rl, 'index_di':index_di, 'index_tr':index_tr}\n",
        "with open(args['indexes'], 'wb') as fp:\n",
        "    pickle.dump(indexes, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#saving the arguements (args)\n",
        "with open(args['args'], 'wb') as fp:\n",
        "    pickle.dump(args, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYhdyr9s7irf"
      },
      "source": [
        "## 3. Dynamic Features with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD7NESmv9G6a"
      },
      "source": [
        "### Loading the data and parameter files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQQQSLGY956p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b9af66-73fb-431b-8881-f1ed2766f0af"
      },
      "source": [
        "# Open from local file\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'args.p'), 'rb') as fp:\n",
        "    args = pickle.load(fp)\n",
        "\n",
        "with open(args['processed_training_vec'], 'rb') as fp:\n",
        "    vec_train = pickle.load(fp)\n",
        "with open(args['processed_test_vec'], 'rb') as fp:\n",
        "    vec_test = pickle.load(fp)\n",
        "    \n",
        "with open(args['weights'], 'rb') as fp:\n",
        "    weights = pickle.load(fp)\n",
        "ac_weights = weights['ac_weights']\n",
        "rl_weights = weights['rl_weights']\n",
        "tr_weights = weights['tr_weights']\n",
        "diagnoses = weights['diagnoses']\n",
        "\n",
        "with open(args['indexes'], 'rb') as fp:\n",
        "    indexes = pickle.load(fp)\n",
        "index_ac = indexes['index_ac']\n",
        "index_rl = indexes['index_rl']\n",
        "index_di = indexes['index_di']\n",
        "index_tr = indexes['index_tr']\n",
        "index_di"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9p9F-tp7irf"
      },
      "source": [
        "# # read from previous\n",
        "# vec_train = args['processed_training_vec']\n",
        "# vec_test = args['processed_test_vec']\n",
        "\n",
        "# weights = args['weights']\n",
        "# ac_weights = weights['ac_weights']\n",
        "# rl_weights = weights['rl_weights']\n",
        "# diagnoses = weights['diagnoses']\n",
        "# tr_weights = weights['tr_weights']\n",
        "\n",
        "# indexes = args['indexes']\n",
        "# index_ac = indexes['index_ac']\n",
        "# index_rl = indexes['index_rl']\n",
        "\n",
        "# index_di = indexes['index_di']\n",
        "# index_di"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwKHlMbb-oyb"
      },
      "source": [
        "### Build Model\n",
        "\n",
        "Author: Bemali Wickramanayake\n",
        "\n",
        "Model Details: This model computes the attention weights for the dynamic 'prefix' features and uses them in the final outcome prediction together with the static features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orcLhhMu8qev",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ebc20b2-a682-42c3-f619-4216836893c3"
      },
      "source": [
        "import keras.layers as L\n",
        "from keras import backend as K\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from keras.layers import Lambda, dot, Activation, concatenate, Dense\n",
        "\n",
        "\n",
        "#Initializing variables\n",
        "vec = vec_train\n",
        "output_folder = MY_WORKSPACE_DIR\n",
        "\n",
        "MAX_LEN = args['n_size']\n",
        "dropout_input = 0.15\n",
        "dropout_context=0.15\n",
        "  # number of lstm cells\n",
        "incl_time = True \n",
        "incl_res = True\n",
        "lstm_size_alpha=args['l_size'] #number of lstm cells\n",
        "lstm_size_beta=args['l_size']\n",
        "print(\"Training prefix-attention model\")\n",
        "\n",
        "l2reg=0.0001\n",
        "\n",
        "output_length = diagnoses\n",
        "\n",
        " \n",
        "  #Inputs include activity, resource and time - time is normalised- 0 mean and unit variance\n",
        "\n",
        "#Dynamic Inputs\n",
        "ac_input = Input(shape=(vec['prefixes']['x_ac_inp'].shape[1], ), name='ac_input') \n",
        "rl_input = Input(shape=(vec['prefixes']['x_rl_inp'].shape[1], ), name='rl_input')\n",
        "t_input = Input(shape=(vec['prefixes']['xt_inp'].shape[1], 1), name='t_input')\n",
        "\n",
        "#static inputs\n",
        "# age_input = Input(shape = (1, ),name = 'age_input') #cl_input_d #vec['static']['x_age_inp']\n",
        "# cl_input_d = Input(shape =(1, ),name = 'cl_input_d') #case length (the duration of a case) in days #vec['static']['x_cl_inp']\n",
        "# cl_input_y = Input(shape=(1, ),name = 'cl_input_y') #case length (the duration of a case) in years, rounded down #vec['static']['x_years_inp']\n",
        "\n",
        "#Activity Embedding - dynamic input 1\n",
        "  \n",
        "ac_embs = L.Embedding(ac_weights.shape[0],\n",
        "                            ac_weights.shape[1],\n",
        "                            weights=[ac_weights], #the one hot encoded activity weight matrix used as the initial weight matrix\n",
        "                            input_length=vec['prefixes']['x_ac_inp'].shape[1],\n",
        "                            trainable=True, name='ac_embedding')(ac_input)\n",
        "\n",
        "dim_ac =ac_weights.shape[1]   \n",
        "\n",
        "\n",
        "\n",
        "#Role Embedding - dynamic input 2\n",
        "rl_embs = Embedding(rl_weights.shape[0],\n",
        "                            rl_weights.shape[1],\n",
        "                            weights=[rl_weights],\n",
        "                            input_length=vec['prefixes']['x_rl_inp'].shape[1],\n",
        "                            trainable=True, name='rl_embedding')(rl_input)\n",
        "\n",
        "dim_rl = rl_weights.shape[1]\n",
        "      \n",
        "\n",
        "#Time input\n",
        "\n",
        "time_embs=t_input\n",
        "dim_t = 1\n",
        "\n",
        "#Concatenated Input Vector\n",
        "\n",
        "full_embs = L.concatenate([ac_embs,rl_embs,time_embs],name = 'full_embs')\n",
        "full_embs = L.Dropout(dropout_input)(full_embs)\n",
        "\n",
        "#Set up the LSTM networks\n",
        "\n",
        "#LSTM \n",
        "alpha = L.Bidirectional(L.CuDNNLSTM(lstm_size_alpha, return_sequences=True),\n",
        "                                    name='alpha')\n",
        "beta = L.Bidirectional(L.LSTM(lstm_size_beta, return_sequences=True),\n",
        "                                   name='beta')\n",
        "\n",
        "\n",
        "#Dense layer for attention\n",
        "alpha_dense = L.Dense(1, kernel_regularizer=l2(l2reg))\n",
        "beta_dense = L.Dense(1,\n",
        "                             activation='tanh', kernel_regularizer=l2(l2reg))\n",
        "\n",
        "#Compute alpha, timestep attention\n",
        "\n",
        "alpha_out = alpha(full_embs)\n",
        "alpha_out = L.TimeDistributed(alpha_dense, name='alpha_dense')(alpha_out)\n",
        "alpha_out = L.Softmax(name='timestep_attention', axis=1)(alpha_out)\n",
        "\n",
        "#Compute beta, feature attention\n",
        "beta_out = beta(full_embs)\n",
        "beta_out = L.TimeDistributed(beta_dense, name='feature_attention')(beta_out)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "#Compute context vector based on attentions and embeddings\n",
        "c_t = L.Multiply()([alpha_out, beta_out,full_embs])\n",
        "c_t = L.Lambda(lambda x: K.sum(x, axis=1))(c_t)\n",
        "\n",
        "  \n",
        "#contexts = L.concatenate([c_t,age_input,cl_input_d], name='contexts')\n",
        "contexts = L.Dropout(dropout_context)(c_t)\n",
        "\n",
        "\n",
        "  \n",
        "act_output = Dense(output_length,\n",
        "                       activation='softmax',\n",
        "                       kernel_initializer='glorot_uniform',\n",
        "                       name='act_output')(contexts)\n",
        "\n",
        "\n",
        "#model = Model(inputs=[ac_input, rl_input, t_input,age_input,cl_input_d], outputs=act_output)\n",
        "model1 = Model(inputs=[ac_input, rl_input, t_input], outputs=act_output)\n",
        "\n",
        "tf.keras.utils.plot_model(\n",
        "    model1,\n",
        "    to_file=\"model1.png\",\n",
        "    show_shapes=False,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv1rN6rONqIG"
      },
      "source": [
        "### Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1HCmT7jXDuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6d1571-2dc6-4ac3-ee77-d0865f752dc3"
      },
      "source": [
        "# Adam\n",
        "\n",
        "args['optim']='Adam'\n",
        "\n",
        "Adam(learning_rate=0.002, beta_1=0.9, beta_2=0.999,\n",
        "                   epsilon=None, decay=0.0, amsgrad=False,name = 'Adam')\n",
        "model1.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBeAZzVEXQ0g"
      },
      "source": [
        "### Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3qXm9mhNqU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be84b5bf-114a-4aee-989c-e4198a6985ed"
      },
      "source": [
        "\n",
        "    \n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "#\n",
        "#    # Output file\n",
        "\n",
        "output_file_path = os.path.join(output_folder,\n",
        "                                    'models/model_rd_' + str(args['n_size']) +\n",
        "                                    ' ' + args['optim'] + '_'  + args['log_name']  +\n",
        "                                    '_{epoch:02d}-{val_loss:.2f}.h5')\n",
        "# print('This is the output file path ', output_file_path)\n",
        "\n",
        "    # Saving\n",
        "model_checkpoint = ModelCheckpoint(output_file_path,\n",
        "                                       monitor='val_loss',\n",
        "                                       verbose=1,\n",
        "                                       save_best_only=True,\n",
        "                                       save_weights_only=False,\n",
        "                                       mode='auto')\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.5,\n",
        "                                   patience=10,\n",
        "                                   verbose=0,\n",
        "                                   mode='auto',\n",
        "                                   min_delta=0.0001,\n",
        "                                   cooldown=0,\n",
        "                                   min_lr=0)\n",
        "\n",
        "model_inputs = [vec['prefixes']['x_ac_inp']]\n",
        "model_inputs.append(vec['prefixes']['x_rl_inp'])\n",
        "model_inputs.append(vec['prefixes']['xt_inp'])\n",
        "#model_inputs.append(vec['static']['x_age_inp'])\n",
        "#model_inputs.append(vec['static']['x_cl_inp'])\n",
        "\n",
        "\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "     sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "history1 = model1.fit(model_inputs,\n",
        "            {'act_output':vec['diagnosis']},\n",
        "            validation_split=0.15,\n",
        "            verbose=1,\n",
        "            callbacks=[early_stopping, model_checkpoint,lr_reducer],\n",
        "            # callbacks=[early_stopping,lr_reducer],\n",
        "            batch_size=64,\n",
        "            epochs=100)\n",
        "#return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OqnnjfhPATs"
      },
      "source": [
        "### Plot accuracy and loss graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykr-h0oKPXGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "479f8cf0-da70-46a4-f46b-16b3987950f4"
      },
      "source": [
        "# list all data in history\n",
        "print(history1.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history1.history['accuracy'])\n",
        "plt.plot(history1.history['val_accuracy'])\n",
        "plt.title('model accuracy (Dynamic)')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history1.history['loss'])\n",
        "plt.plot(history1.history['val_loss'])\n",
        "plt.title('model loss (Dynamic)')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnrepKAgQZO6"
      },
      "source": [
        "### Evaluating the Prediction performance of Diagnosis for new samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bAFKRrHTI19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c463b510-c877-45b9-cdc1-061e2e6d0f87"
      },
      "source": [
        "# Generating Inputs\n",
        "\n",
        "x_test = [vec_test['prefixes']['x_ac_inp']]\n",
        "x_test.append(vec_test['prefixes']['x_rl_inp'])\n",
        "x_test.append(vec_test['prefixes']['xt_inp'])\n",
        "#x_test.append(vec_test['static']['x_age_inp'])\n",
        "#x_test.append(vec_test['static']['x_cl_inp'])\n",
        "\n",
        "\n",
        "y_test = vec_test['diagnosis']\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model1.evaluate(x_test, y_test, batch_size=10)# fix the padding size to be common for both test and train\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "#Prediction\n",
        "\n",
        "y_pred = model1.predict(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3tQ3B8jVe-z"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ63y5-hQbSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "df775b68-afd4-47fd-bf2c-df1f2344273a"
      },
      "source": [
        "#Confusion Matrix\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(matrix)\n",
        "df_cm = pd.DataFrame(matrix, index = [index_di[i] for i in range(5)],\n",
        "                  columns = [index_di[i] for i in range(5)])\n",
        "\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDR_crdc7irf"
      },
      "source": [
        "## 4. Dynamic and Static Features with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyXsCT-ANeid"
      },
      "source": [
        "### Loading the data and parameter files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwUysL8PNeie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634ab4b4-f1f9-40e0-a643-8debda81aacb"
      },
      "source": [
        "# Open from local file\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'args.p'), 'rb') as fp:\n",
        "    args = pickle.load(fp)\n",
        "\n",
        "with open(args['processed_training_vec'], 'rb') as fp:\n",
        "    vec_train = pickle.load(fp)\n",
        "with open(args['processed_test_vec'], 'rb') as fp:\n",
        "    vec_test = pickle.load(fp)\n",
        "    \n",
        "with open(args['weights'], 'rb') as fp:\n",
        "    weights = pickle.load(fp)\n",
        "ac_weights = weights['ac_weights']\n",
        "rl_weights = weights['rl_weights']\n",
        "tr_weights = weights['tr_weights']\n",
        "diagnoses = weights['diagnoses']\n",
        "\n",
        "with open(args['indexes'], 'rb') as fp:\n",
        "    indexes = pickle.load(fp)\n",
        "index_ac = indexes['index_ac']\n",
        "index_rl = indexes['index_rl']\n",
        "index_di = indexes['index_di']\n",
        "index_tr = indexes['index_tr']\n",
        "index_di"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zynCUfsmNeie"
      },
      "source": [
        "# # read from previous\n",
        "# vec_train = args['processed_training_vec']\n",
        "# vec_test = args['processed_test_vec']\n",
        "\n",
        "# weights = args['weights']\n",
        "# ac_weights = weights['ac_weights']\n",
        "# rl_weights = weights['rl_weights']\n",
        "# diagnoses = weights['diagnoses']\n",
        "\n",
        "# indexes = args['indexes']\n",
        "# index_ac = indexes['index_ac']\n",
        "# index_rl = indexes['index_rl']\n",
        "\n",
        "# index_di = indexes['index_di']\n",
        "# index_di"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTs2TT9xNhc0"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hRjPdhTNZvN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5702162-b66b-48ce-d58a-bd93548e8de7"
      },
      "source": [
        "import keras.layers as L\n",
        "from keras import backend as K\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from keras.layers import Lambda, dot, Activation, concatenate, Dense\n",
        "\n",
        "\n",
        "#Initializing variables\n",
        "vec = vec_train\n",
        "output_folder = MY_WORKSPACE_DIR\n",
        "\n",
        "MAX_LEN = args['n_size']\n",
        "dropout_input = 0.15\n",
        "dropout_context=0.15\n",
        "  # number of lstm cells\n",
        "incl_time = True \n",
        "incl_res = True\n",
        "lstm_size_alpha=args['l_size'] #number of lstm cells\n",
        "lstm_size_beta=args['l_size']\n",
        "print(\"Training prefix-attention model\")\n",
        "\n",
        "l2reg=0.0001\n",
        "\n",
        "output_length = diagnoses\n",
        "\n",
        " \n",
        "  #Inputs include activity, resource and time - time is normalised- 0 mean and unit variance\n",
        "\n",
        "#Dynamic Inputs\n",
        "ac_input = Input(shape=(vec['prefixes']['x_ac_inp'].shape[1], ), name='ac_input') \n",
        "rl_input = Input(shape=(vec['prefixes']['x_rl_inp'].shape[1], ), name='rl_input')\n",
        "t_input = Input(shape=(vec['prefixes']['xt_inp'].shape[1], 1), name='t_input')\n",
        "\n",
        "#static inputs\n",
        "age_input = Input(shape = (1, ),name = 'age_input') #cl_input_d #vec['static']['x_age_inp']\n",
        "# cl_input_d = Input(shape =(1, ),name = 'cl_input_d') #case length (the duration of a case) in days #vec['static']['x_cl_inp']\n",
        "# cl_input_y = Input(shape=(1, ),name = 'cl_input_y') #case length (the duration of a case) in years, rounded down #vec['static']['x_years_inp']\n",
        "tr_input = Input(shape=(1, ),name = 'tr_input')\n",
        "\n",
        "\n",
        "#Activity Embedding - dynamic input 1\n",
        "  \n",
        "ac_embs = L.Embedding(ac_weights.shape[0],\n",
        "                            ac_weights.shape[1],\n",
        "                            weights=[ac_weights], #the one hot encoded activity weight matrix used as the initial weight matrix\n",
        "                            input_length=vec['prefixes']['x_ac_inp'].shape[1],\n",
        "                            trainable=True, name='ac_embedding')(ac_input)\n",
        "\n",
        "dim_ac =ac_weights.shape[1]   \n",
        "\n",
        "\n",
        "\n",
        "#Role Embedding - dynamic input 2\n",
        "rl_embs = Embedding(rl_weights.shape[0],\n",
        "                            rl_weights.shape[1],\n",
        "                            weights=[rl_weights],\n",
        "                            input_length=vec['prefixes']['x_rl_inp'].shape[1],\n",
        "                            trainable=True, name='rl_embedding')(rl_input)\n",
        "\n",
        "dim_rl = rl_weights.shape[1]\n",
        "\n",
        "\n",
        "#Time input\n",
        "\n",
        "time_embs=t_input\n",
        "dim_t = 1\n",
        "\n",
        "#Concatenated Input Vector\n",
        "\n",
        "full_embs = L.concatenate([ac_embs,rl_embs,time_embs],name = 'full_embs')\n",
        "full_embs = L.Dropout(dropout_input)(full_embs)\n",
        "\n",
        "#Set up the LSTM networks\n",
        "\n",
        "#LSTM \n",
        "alpha = L.Bidirectional(L.CuDNNLSTM(lstm_size_alpha, return_sequences=True),\n",
        "                                    name='alpha')\n",
        "beta = L.Bidirectional(L.LSTM(lstm_size_beta, return_sequences=True),\n",
        "                                   name='beta')\n",
        "\n",
        "\n",
        "#Dense layer for attention\n",
        "alpha_dense = L.Dense(1, kernel_regularizer=l2(l2reg))\n",
        "beta_dense = L.Dense(1,activation='tanh', kernel_regularizer=l2(l2reg))\n",
        "\n",
        "#Compute alpha, timestep attention\n",
        "\n",
        "alpha_out = alpha(full_embs)\n",
        "alpha_out = L.TimeDistributed(alpha_dense, name='alpha_dense')(alpha_out)\n",
        "alpha_out = L.Softmax(name='timestep_attention', axis=1)(alpha_out)\n",
        "\n",
        "#Compute beta, feature attention\n",
        "beta_out = beta(full_embs)\n",
        "beta_out = L.TimeDistributed(beta_dense, name='feature_attention')(beta_out)\n",
        "\n",
        "  \n",
        "#Compute context vector based on attentions and embeddings\n",
        "c_t = L.Multiply()([alpha_out, beta_out,full_embs])\n",
        "c_t = L.Lambda(lambda x: K.sum(x, axis=1))(c_t)\n",
        "\n",
        "\n",
        "# Static Features with attention\n",
        "\n",
        "#Treatment Embedding - static input\n",
        "# tr_embs = L.Embedding(tr_weights.shape[0],\n",
        "#                             tr_weights.shape[1],\n",
        "#                             weights=[tr_weights], #the one hot encoded activity weight matrix used as the initial weight matrix\n",
        "#                             input_length=len(vec['static']['x_tr_inp']),\n",
        "#                             trainable=True, name='tr_embedding')(tr_input)\n",
        "\n",
        "# dim_tr =tr_weights.shape[1]   \n",
        "\n",
        "#Dense layer for attention of static features\n",
        "\n",
        "alpha_dense_age = L.Dense(1, kernel_regularizer=l2(l2reg),name = 'alpha_dense_age')\n",
        "# alpha_dense_cl = L.Dense(1, kernel_regularizer=l2(l2reg),name = 'alpha_dense_cl')\n",
        "alpha_dense_tr = L.Dense(1, kernel_regularizer=l2(l2reg),name = 'alpha_dense_tr')\n",
        "\n",
        "#Compute static attentions\n",
        "\n",
        "age_out = alpha_dense_age(age_input)\n",
        "age_out = L.Softmax(name='age_static_attention', axis=1)(age_out)\n",
        "\n",
        "# cl_out = alpha_dense_cl(age_input)\n",
        "# cl_out = L.Softmax(name='cl_static_attention', axis=1)(cl_input_d)\n",
        "\n",
        "tr_out = alpha_dense_tr(tr_input)\n",
        "tr_out = L.Softmax(name='tr_static_attention', axis=1)(tr_out)\n",
        "\n",
        "\n",
        "  \n",
        "contexts = L.concatenate([c_t,age_out,tr_out], name='contexts')\n",
        "contexts = L.Dropout(dropout_context)(contexts)\n",
        "\n",
        "\n",
        "  \n",
        "act_output = Dense(output_length,\n",
        "                       activation='softmax',\n",
        "                       kernel_initializer='glorot_uniform',\n",
        "                       name='act_output')(contexts)\n",
        "\n",
        " #changing the optimizer\n",
        " #----- delete this bit if works--------\n",
        "#args['optim'] = 'Adagrad'\n",
        " #------------------------------------\n",
        "\n",
        "model2 = Model(inputs=[ac_input, rl_input, t_input,age_input,tr_input], outputs=act_output)\n",
        "#model = Model(inputs=[ac_input, rl_input, t_input], outputs=act_output)\n",
        "\n",
        "tf.keras.utils.plot_model(\n",
        "    model2,\n",
        "    to_file=\"model2.png\",\n",
        "    show_shapes=False,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M07_NNjHXXmQ"
      },
      "source": [
        "### Compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ3c0Xw0XdHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa898b6-3a2c-41c7-cbff-9b3559dd210c"
      },
      "source": [
        "args['optim']='Adam'\n",
        "\n",
        "Adam(learning_rate=0.002, beta_1=0.9, beta_2=0.999,\n",
        "                   epsilon=None, decay=0.0, amsgrad=False,name = 'Adam')\n",
        "model2.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQyvyQrXWB4F"
      },
      "source": [
        "### Fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIxhTureVx7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e4ee9a1-5f76-4401-e6f7-f4e3c8cf32e0"
      },
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "#\n",
        "#    # Output file\n",
        "output_file_path = os.path.join(output_folder,\n",
        "                                    'models/model_rd_' + str(args['n_size']) +\n",
        "                                    ' ' + args['optim'] + '_'  + args['log_name']  +\n",
        "                                    '_{epoch:02d}-{val_loss:.2f}.h5')\n",
        "# print('This is the output file path ', output_file_path)\n",
        "    # Saving\n",
        "model_checkpoint = ModelCheckpoint(output_file_path,\n",
        "                                       monitor='val_loss',\n",
        "                                       verbose=1,\n",
        "                                       save_best_only=True,\n",
        "                                       save_weights_only=False,\n",
        "                                       mode='auto')\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.5,\n",
        "                                   patience=10,\n",
        "                                   verbose=0,\n",
        "                                   mode='auto',\n",
        "                                   min_delta=0.0001,\n",
        "                                   cooldown=0,\n",
        "                                   min_lr=0)\n",
        "\n",
        "model_inputs = [vec['prefixes']['x_ac_inp']]\n",
        "model_inputs.append(vec['prefixes']['x_rl_inp'])\n",
        "model_inputs.append(vec['prefixes']['xt_inp'])\n",
        "model_inputs.append(vec['static']['x_age_inp'])\n",
        "model_inputs.append(vec['static']['x_cl_inp'])\n",
        "\n",
        "\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "     sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "history2 = model2.fit(model_inputs,\n",
        "            {'act_output':vec['diagnosis']},\n",
        "            validation_split=0.15,\n",
        "            verbose=1,\n",
        "            callbacks=[early_stopping, model_checkpoint,lr_reducer],\n",
        "            # callbacks=[early_stopping,lr_reducer],\n",
        "            batch_size=64,\n",
        "            epochs=100)\n",
        "#return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pduAMCiWb2B"
      },
      "source": [
        "### Plot accuracy and loss graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89INqZGwWhl8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "d10d3375-dd37-4740-a7a1-005e7e1a2bf1"
      },
      "source": [
        "# list all data in history\n",
        "print(history2.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history2.history['accuracy'])\n",
        "plt.plot(history2.history['val_accuracy'])\n",
        "plt.title('model accuracy (Dynamic and Static)')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history2.history['val_loss'])\n",
        "plt.title('model loss (Dynamic and Static)')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK5UJh1SXwxx"
      },
      "source": [
        "### Evaluating the Prediction performance of Diagnosis for new samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zFxNQRWX2sJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1376b488-e94f-4bbb-838f-9ec77b8fc37f"
      },
      "source": [
        "# Generating Inputs\n",
        "\n",
        "x_test = [vec_test['prefixes']['x_ac_inp']]\n",
        "x_test.append(vec_test['prefixes']['x_rl_inp'])\n",
        "x_test.append(vec_test['prefixes']['xt_inp'])\n",
        "x_test.append(vec_test['static']['x_age_inp'])\n",
        "x_test.append(vec_test['static']['x_cl_inp'])\n",
        "\n",
        "\n",
        "y_test = vec_test['diagnosis']\n",
        "\n",
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model2.evaluate(x_test, y_test, batch_size=10)# fix the padding size to be common for both test and train\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "#Prediction\n",
        "\n",
        "y_pred = model2.predict(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbzUKissX6LX"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OMK3VvTX4YQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "c39fca04-4728-45ec-c795-1aa3bf99c728"
      },
      "source": [
        "#Confusion Matrix\n",
        "\n",
        "import seaborn as sn\n",
        "\n",
        "\n",
        "matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(matrix)\n",
        "df_cm = pd.DataFrame(matrix, index = [index_di[i] for i in range(5)],\n",
        "                  columns = [index_di[i] for i in range(5)])\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjCdhtdsX9mZ"
      },
      "source": [
        "### Visualizing Attention Weights\n",
        "\n",
        "credits: https://stackoverflow.com/questions/53867351/how-to-visualize-attention-weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5sNYlUQX9vp"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}